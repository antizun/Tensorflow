{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Multi Capa con LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$i_t = \\sigma(B_ih_{t-1}+A_ix_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$C_t = \\tanh(B_ch_{t-1}+A_cx_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f_t = \\sigma(B_fh_{t-1}+A_fx_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$N_t = i_t\\cdot C_t +f_t N_{t-1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$O_t = \\sigma(B_oh_{t-1}+A_ox_t+D_oN_t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$h_t = O_t\\cdot \\tanh(N_t)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import requests\n",
    "import numpy as np\n",
    "import collections\n",
    "import random\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../datasets/shakespeare\"\n",
    "data_file = \"shakespeare.txt\"\n",
    "model_path = \"shakespeare_model\"\n",
    "full_model_dir = os.path.join(data_dir, model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&()*+,./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punctuation = ''.join([x for x in string.punctuation if x not in ['-', \"'\"]])\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(full_model_dir):\n",
    "    os.makedirs(full_model_dir)\n",
    "\n",
    "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
    "    url = \"http://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
    "    response = requests.get(url)\n",
    "    s_text = response.content.decode('utf-8')\n",
    "    s_text = s_text[7675:]\n",
    "    s_text = s_text.replace(\"\\r\\n\", '')\n",
    "    s_text = s_text.replace(\"\\n\",'')\n",
    "    \n",
    "    with open(os.path.join(data_dir, data_file), \"w\") as file_out:\n",
    "        file_out.write(s_text)\n",
    "else:\n",
    "    with open(os.path.join(data_dir, data_file), \"r\") as file_out:\n",
    "        s_text = file_out.read().replace(\"\\n\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
    "s_text = re.sub('\\s+', ' ', s_text).strip().lower()\n",
    "\n",
    "char_list = list(s_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "\n",
    "num_layers = 3\n",
    "min_word_freq = 5\n",
    "rnn_size = 128\n",
    "\n",
    "epoch = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "training_seq_len = 50\n",
    "embedding_size = rnn_size\n",
    "save_every = 500\n",
    "eval_every = 50\n",
    "\n",
    "prime_texts = ['to be or not be', 'thou art more', 'wherefore art thou']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocabulary(text, min_word_freq):\n",
    "    word_count = collections.Counter(text.split(' '))\n",
    "    word_count = {key:val for key, val in word_count.items() if val>min_word_freq}\n",
    "    words = word_count.keys()\n",
    "    word_to_vec = {key:(ix+1) for ix, key in enumerate(words)}\n",
    "    word_to_vec['unknown'] = 0\n",
    "    vec_to_word = {val:key for key, val in word_to_vec.items()}\n",
    "    return vec_to_word, word_to_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec2word, word2vec = build_vocabulary(s_text, min_word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(vec2word)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8009"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_text_words = s_text.split(' ')\n",
    "s_text_vec = []\n",
    "for ix, x in enumerate(s_text_words):\n",
    "    try: \n",
    "        s_text_vec.append(word2vec[x])\n",
    "    except:\n",
    "        s_text_vec.append(0)\n",
    "s_text_vec = np.array(s_text_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Model():\n",
    "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate, ## embedding_size\n",
    "                 training_seq_length, vocabulary_size, infer = False ):\n",
    "        \n",
    "        self.embedding_size = embedding_size ### guardar en self el objeto embedding_size\n",
    "        \n",
    "        self.rnn_size = rnn_size\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.infer = infer\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        if infer:\n",
    "            self.batch_size = 1\n",
    "            self.training_seq_len = 1\n",
    "        else:\n",
    "            self.batch_size = batch_size\n",
    "            self.training_seq_len = training_seq_len\n",
    "            \n",
    "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size) ### cambiado por contrib!!!\n",
    "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
    "        \n",
    "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
    "\n",
    "        \n",
    "        with tf.variable_scope('lstm_vars'):\n",
    "            weigths = tf.get_variable('weigths', [self.rnn_size, self.vocabulary_size], tf.float32,\n",
    "                                tf.random_normal_initializer())\n",
    "            bias = tf.get_variable('bias', [self.vocabulary_size], tf.float32,\n",
    "                                tf.constant_initializer(0.0))\n",
    "            \n",
    "            embedding_mat = tf.get_variable('embedding_mat', [self.vocabulary_size, self.embedding_size],\n",
    "                                            tf.float32, tf.random_normal_initializer())\n",
    "            \n",
    "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
    "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
    "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
    "            \n",
    "        def inferred_loop(prev, count):\n",
    "            prev_trans = tf.add(tf.matmul(prev, weights), bias)\n",
    "            prev_symbol = tf.stop_gradient(tf.argmax(prev_trans, 1))\n",
    "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
    "            \n",
    "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
    "        outputs, last_state = decoder(rnn_inputs_trimmed, ### FALTABA UNA S\n",
    "                                      self.initial_state,\n",
    "                                      self.lstm_cell,\n",
    "                                      loop_function=inferred_loop if infer else None)\n",
    "        \n",
    "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
    "        \n",
    "        self.logit_output = tf.add(tf.matmul(output, weigths),bias)\n",
    "        self.model_output = tf.nn.softmax(self.logit_output)\n",
    "        \n",
    "        loss_func = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
    "        loss = loss_func([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
    "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
    "                self.vocabulary_size)\n",
    "        \n",
    "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
    "        self.final_state = last_state\n",
    "\n",
    "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
    "        \n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
    "        self.train_step = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "\n",
    "        \n",
    "        \n",
    "    def sample(self, session, words = vec2word, vocab = word2vec, num = 10, prime_text = 'thou art'):\n",
    "        state = session.run(self.lstm_cell.zero_state(1, tf.float32))\n",
    "        word_list = prime_text.split()\n",
    "        for word in word_list[:-1]:\n",
    "            x = np.zeros((1,1))\n",
    "            x[0,0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
    "            [state] = session.run([self.final_state], feed_dict=feed_dict)\n",
    "        \n",
    "        out_sentence = prime_text\n",
    "        word = word_list[-1]\n",
    "        for n in range(num):\n",
    "            x = np.zeros((1,1))\n",
    "            x[0,0] = vocab[word]\n",
    "            feed_dict = {self.x_data: x, self.initial_state: state}\n",
    "            [model_output, state] = session.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
    "            sample = np.argmax(model_output[0])\n",
    "            if sample == 0:\n",
    "                break\n",
    "            word = words[sample]\n",
    "            out_sentence = out_sentence + \" \" + word\n",
    "        return out_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch actual : 1 / 10\n",
      "Iteración: 10, Epoch: 1, Batch: 10 de 182, Pérdida: 9.79\n",
      "Iteración: 20, Epoch: 1, Batch: 20 de 182, Pérdida: 9.23\n",
      "Iteración: 30, Epoch: 1, Batch: 30 de 182, Pérdida: 8.87\n",
      "Iteración: 40, Epoch: 1, Batch: 40 de 182, Pérdida: 8.51\n",
      "Iteración: 50, Epoch: 1, Batch: 50 de 182, Pérdida: 8.09\n",
      "to be or not be\n",
      "thou art more like in the\n",
      "wherefore art thou yes passions whispers whispers antony not\n",
      "Iteración: 60, Epoch: 1, Batch: 60 de 182, Pérdida: 8.02\n",
      "Iteración: 70, Epoch: 1, Batch: 70 de 182, Pérdida: 7.63\n",
      "Iteración: 80, Epoch: 1, Batch: 80 de 182, Pérdida: 7.39\n",
      "Iteración: 90, Epoch: 1, Batch: 90 de 182, Pérdida: 7.41\n",
      "Iteración: 100, Epoch: 1, Batch: 100 de 182, Pérdida: 7.11\n",
      "to be or not be\n",
      "thou art more than in thy\n",
      "wherefore art thou casket moon boar's pisanio patient and\n",
      "Iteración: 110, Epoch: 1, Batch: 110 de 182, Pérdida: 6.95\n",
      "Iteración: 120, Epoch: 1, Batch: 120 de 182, Pérdida: 6.80\n",
      "Iteración: 130, Epoch: 1, Batch: 130 de 182, Pérdida: 6.93\n",
      "Iteración: 140, Epoch: 1, Batch: 140 de 182, Pérdida: 6.63\n",
      "Iteración: 150, Epoch: 1, Batch: 150 de 182, Pérdida: 6.69\n",
      "to be or not be\n",
      "thou art more than from the\n",
      "wherefore art thou casket moon experience must\n",
      "Iteración: 160, Epoch: 1, Batch: 160 de 182, Pérdida: 6.60\n",
      "Iteración: 170, Epoch: 1, Batch: 170 de 182, Pérdida: 6.76\n",
      "Iteración: 180, Epoch: 1, Batch: 180 de 182, Pérdida: 6.61\n",
      "Epoch actual : 2 / 10\n",
      "Iteración: 190, Epoch: 2, Batch: 9 de 182, Pérdida: 7.14\n",
      "Iteración: 200, Epoch: 2, Batch: 19 de 182, Pérdida: 6.80\n",
      "to be or not be\n",
      "thou art more than from the\n",
      "wherefore art thou casket moon experience must but of the\n",
      "Iteración: 210, Epoch: 2, Batch: 29 de 182, Pérdida: 7.00\n",
      "Iteración: 220, Epoch: 2, Batch: 39 de 182, Pérdida: 6.26\n",
      "Iteración: 230, Epoch: 2, Batch: 49 de 182, Pérdida: 6.13\n",
      "Iteración: 240, Epoch: 2, Batch: 59 de 182, Pérdida: 6.30\n",
      "Iteración: 250, Epoch: 2, Batch: 69 de 182, Pérdida: 6.61\n",
      "to be or not be\n",
      "thou art more than from the\n",
      "wherefore art thou casket moon experience must but but that i have a\n",
      "Iteración: 260, Epoch: 2, Batch: 79 de 182, Pérdida: 6.15\n",
      "Iteración: 270, Epoch: 2, Batch: 89 de 182, Pérdida: 6.25\n",
      "Iteración: 280, Epoch: 2, Batch: 99 de 182, Pérdida: 6.55\n",
      "Iteración: 290, Epoch: 2, Batch: 109 de 182, Pérdida: 6.42\n",
      "Iteración: 300, Epoch: 2, Batch: 119 de 182, Pérdida: 6.36\n",
      "to be or not be\n",
      "thou art more than the\n",
      "wherefore art thou milk shouldst gallop art milk morrow a\n",
      "Iteración: 310, Epoch: 2, Batch: 129 de 182, Pérdida: 6.19\n",
      "Iteración: 320, Epoch: 2, Batch: 139 de 182, Pérdida: 6.44\n",
      "Iteración: 330, Epoch: 2, Batch: 149 de 182, Pérdida: 6.37\n",
      "Iteración: 340, Epoch: 2, Batch: 159 de 182, Pérdida: 5.94\n",
      "Iteración: 350, Epoch: 2, Batch: 169 de 182, Pérdida: 6.21\n",
      "to be or not be\n",
      "thou art more than thou\n",
      "wherefore art thou navy milk nourish mistresses thrift there's there's there's copies pursuit\n",
      "Iteración: 360, Epoch: 2, Batch: 179 de 182, Pérdida: 6.06\n",
      "Epoch actual : 3 / 10\n",
      "Iteración: 370, Epoch: 3, Batch: 8 de 182, Pérdida: 6.18\n",
      "Iteración: 380, Epoch: 3, Batch: 18 de 182, Pérdida: 6.15\n",
      "Iteración: 390, Epoch: 3, Batch: 28 de 182, Pérdida: 6.32\n",
      "Iteración: 400, Epoch: 3, Batch: 38 de 182, Pérdida: 6.24\n",
      "to be or not be\n",
      "thou art more than than\n",
      "wherefore art thou navy unrest worst beheld yours yours gone no more than\n",
      "Iteración: 410, Epoch: 3, Batch: 48 de 182, Pérdida: 6.04\n",
      "Iteración: 420, Epoch: 3, Batch: 58 de 182, Pérdida: 5.94\n",
      "Iteración: 430, Epoch: 3, Batch: 68 de 182, Pérdida: 6.42\n",
      "Iteración: 440, Epoch: 3, Batch: 78 de 182, Pérdida: 6.17\n",
      "Iteración: 450, Epoch: 3, Batch: 88 de 182, Pérdida: 6.14\n",
      "to be or not be\n",
      "thou art more than than\n",
      "wherefore art thou milk shouldst gallop art milk nought being duty utt'red jest\n",
      "Iteración: 460, Epoch: 3, Batch: 98 de 182, Pérdida: 6.19\n",
      "Iteración: 470, Epoch: 3, Batch: 108 de 182, Pérdida: 6.23\n",
      "Iteración: 480, Epoch: 3, Batch: 118 de 182, Pérdida: 6.24\n",
      "Iteración: 490, Epoch: 3, Batch: 128 de 182, Pérdida: 6.28\n",
      "Iteración: 500, Epoch: 3, Batch: 138 de 182, Pérdida: 6.06\n",
      "Modelo guardado correctamente en ../../datasets/shakespeare/shakespeare_model/model\n",
      "to be or not be\n",
      "thou art more than than the\n",
      "wherefore art thou navy unrest worst beheld yours yours yours gone no no\n",
      "Iteración: 510, Epoch: 3, Batch: 148 de 182, Pérdida: 6.20\n",
      "Iteración: 520, Epoch: 3, Batch: 158 de 182, Pérdida: 6.10\n",
      "Iteración: 530, Epoch: 3, Batch: 168 de 182, Pérdida: 5.97\n",
      "Iteración: 540, Epoch: 3, Batch: 178 de 182, Pérdida: 6.31\n",
      "Epoch actual : 4 / 10\n",
      "Iteración: 550, Epoch: 4, Batch: 7 de 182, Pérdida: 6.19\n",
      "to be or not be\n",
      "thou art more than than the\n",
      "wherefore art thou navy unrest worst beheld yours yours yours gone no no\n",
      "Iteración: 560, Epoch: 4, Batch: 17 de 182, Pérdida: 6.27\n",
      "Iteración: 570, Epoch: 4, Batch: 27 de 182, Pérdida: 6.22\n",
      "Iteración: 580, Epoch: 4, Batch: 37 de 182, Pérdida: 5.99\n",
      "Iteración: 590, Epoch: 4, Batch: 47 de 182, Pérdida: 6.10\n",
      "Iteración: 600, Epoch: 4, Batch: 57 de 182, Pérdida: 5.81\n",
      "to be or not be\n",
      "thou art more than than the\n",
      "wherefore art thou shouldst shouldst unusual surgeon midst bashful brains taper worst distract\n",
      "Iteración: 610, Epoch: 4, Batch: 67 de 182, Pérdida: 6.15\n",
      "Iteración: 620, Epoch: 4, Batch: 77 de 182, Pérdida: 6.17\n",
      "Iteración: 630, Epoch: 4, Batch: 87 de 182, Pérdida: 5.89\n",
      "Iteración: 640, Epoch: 4, Batch: 97 de 182, Pérdida: 6.33\n",
      "Iteración: 650, Epoch: 4, Batch: 107 de 182, Pérdida: 5.96\n",
      "to be or not be\n",
      "thou art more than than the\n",
      "wherefore art thou shouldst art gallop began false false false royal\n",
      "Iteración: 660, Epoch: 4, Batch: 117 de 182, Pérdida: 6.15\n",
      "Iteración: 670, Epoch: 4, Batch: 127 de 182, Pérdida: 6.05\n",
      "Iteración: 680, Epoch: 4, Batch: 137 de 182, Pérdida: 6.11\n",
      "Iteración: 690, Epoch: 4, Batch: 147 de 182, Pérdida: 6.00\n",
      "Iteración: 700, Epoch: 4, Batch: 157 de 182, Pérdida: 6.06\n",
      "to be or not be\n",
      "thou art more than than the\n",
      "wherefore art thou shouldst art knew'st perfect nourish mistresses thrift happier to the\n",
      "Iteración: 710, Epoch: 4, Batch: 167 de 182, Pérdida: 6.00\n",
      "Iteración: 720, Epoch: 4, Batch: 177 de 182, Pérdida: 5.99\n",
      "Epoch actual : 5 / 10\n",
      "Iteración: 730, Epoch: 5, Batch: 6 de 182, Pérdida: 5.84\n",
      "Iteración: 740, Epoch: 5, Batch: 16 de 182, Pérdida: 6.07\n",
      "Iteración: 750, Epoch: 5, Batch: 26 de 182, Pérdida: 6.24\n",
      "to be or not be\n",
      "thou art more than than the\n",
      "wherefore art thou shouldst art gallop scorn'd authority after prince of the\n",
      "Iteración: 760, Epoch: 5, Batch: 36 de 182, Pérdida: 6.16\n",
      "Iteración: 770, Epoch: 5, Batch: 46 de 182, Pérdida: 6.12\n",
      "Iteración: 780, Epoch: 5, Batch: 56 de 182, Pérdida: 6.03\n",
      "Iteración: 790, Epoch: 5, Batch: 66 de 182, Pérdida: 6.23\n",
      "Iteración: 800, Epoch: 5, Batch: 76 de 182, Pérdida: 6.12\n",
      "to be or not be\n",
      "thou art more than than the\n",
      "wherefore art thou shouldst art knew'st perfect nourish enrolled bedistributed so long as\n",
      "Iteración: 810, Epoch: 5, Batch: 86 de 182, Pérdida: 6.14\n",
      "Iteración: 820, Epoch: 5, Batch: 96 de 182, Pérdida: 6.02\n",
      "Iteración: 830, Epoch: 5, Batch: 106 de 182, Pérdida: 5.92\n",
      "Iteración: 840, Epoch: 5, Batch: 116 de 182, Pérdida: 5.98\n",
      "Iteración: 850, Epoch: 5, Batch: 126 de 182, Pérdida: 6.04\n",
      "to be or not be\n",
      "thou art more than the\n",
      "wherefore art thou shouldst art knew'st perfect nourish bedistributed so long as such\n",
      "Iteración: 860, Epoch: 5, Batch: 136 de 182, Pérdida: 6.26\n",
      "Iteración: 870, Epoch: 5, Batch: 146 de 182, Pérdida: 5.90\n",
      "Iteración: 880, Epoch: 5, Batch: 156 de 182, Pérdida: 5.96\n",
      "Iteración: 890, Epoch: 5, Batch: 166 de 182, Pérdida: 5.97\n",
      "Iteración: 900, Epoch: 5, Batch: 176 de 182, Pérdida: 5.76\n",
      "to be or not be\n",
      "thou art more than than the\n",
      "wherefore art thou shouldst art knew'st perfect nourish bedistributed so long as such\n",
      "Epoch actual : 6 / 10\n",
      "Iteración: 910, Epoch: 6, Batch: 5 de 182, Pérdida: 5.86\n",
      "Iteración: 920, Epoch: 6, Batch: 15 de 182, Pérdida: 5.97\n",
      "Iteración: 930, Epoch: 6, Batch: 25 de 182, Pérdida: 5.92\n",
      "Iteración: 940, Epoch: 6, Batch: 35 de 182, Pérdida: 5.87\n",
      "Iteración: 950, Epoch: 6, Batch: 45 de 182, Pérdida: 5.98\n",
      "to be or not be\n",
      "thou art more than than the\n",
      "wherefore art thou shouldst art knew'st perfect nourish bedistributed tiger parrot writ shall\n",
      "Iteración: 960, Epoch: 6, Batch: 55 de 182, Pérdida: 5.96\n",
      "Iteración: 970, Epoch: 6, Batch: 65 de 182, Pérdida: 6.08\n",
      "Iteración: 980, Epoch: 6, Batch: 75 de 182, Pérdida: 6.23\n",
      "Iteración: 990, Epoch: 6, Batch: 85 de 182, Pérdida: 5.89\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteración: 1000, Epoch: 6, Batch: 95 de 182, Pérdida: 5.92\n",
      "Modelo guardado correctamente en ../../datasets/shakespeare/shakespeare_model/model\n",
      "to be or not be\n",
      "thou art more than\n",
      "wherefore art thou shouldst art restrain'd suff'ring quietly airy airy whet wet immediately\n",
      "Iteración: 1010, Epoch: 6, Batch: 105 de 182, Pérdida: 6.13\n",
      "Iteración: 1020, Epoch: 6, Batch: 115 de 182, Pérdida: 5.95\n",
      "Iteración: 1030, Epoch: 6, Batch: 125 de 182, Pérdida: 6.05\n",
      "Iteración: 1040, Epoch: 6, Batch: 135 de 182, Pérdida: 6.10\n",
      "Iteración: 1050, Epoch: 6, Batch: 145 de 182, Pérdida: 5.67\n",
      "to be or not be\n",
      "thou art more than\n",
      "wherefore art thou shouldst art knew'st perfect nourish bedistributed so long as such\n",
      "Iteración: 1060, Epoch: 6, Batch: 155 de 182, Pérdida: 5.97\n",
      "Iteración: 1070, Epoch: 6, Batch: 165 de 182, Pérdida: 5.89\n",
      "Iteración: 1080, Epoch: 6, Batch: 175 de 182, Pérdida: 5.96\n",
      "Epoch actual : 7 / 10\n",
      "Iteración: 1090, Epoch: 7, Batch: 4 de 182, Pérdida: 5.94\n",
      "Iteración: 1100, Epoch: 7, Batch: 14 de 182, Pérdida: 5.77\n",
      "to be or not be\n",
      "thou art more than than a\n",
      "wherefore art thou shouldst art knew'st perfect nourish bedistributed so long as such\n",
      "Iteración: 1110, Epoch: 7, Batch: 24 de 182, Pérdida: 6.17\n",
      "Iteración: 1120, Epoch: 7, Batch: 34 de 182, Pérdida: 5.84\n",
      "Iteración: 1130, Epoch: 7, Batch: 44 de 182, Pérdida: 5.82\n",
      "Iteración: 1140, Epoch: 7, Batch: 54 de 182, Pérdida: 5.90\n",
      "Iteración: 1150, Epoch: 7, Batch: 64 de 182, Pérdida: 6.00\n",
      "to be or not be\n",
      "thou art more than\n",
      "wherefore art thou shouldst art restrain'd suff'ring breathless wave toil stands worst mean'st\n",
      "Iteración: 1160, Epoch: 7, Batch: 74 de 182, Pérdida: 5.92\n",
      "Iteración: 1170, Epoch: 7, Batch: 84 de 182, Pérdida: 5.87\n",
      "Iteración: 1180, Epoch: 7, Batch: 94 de 182, Pérdida: 5.69\n",
      "Iteración: 1190, Epoch: 7, Batch: 104 de 182, Pérdida: 5.81\n",
      "Iteración: 1200, Epoch: 7, Batch: 114 de 182, Pérdida: 5.83\n",
      "to be or not be\n",
      "thou art more than\n",
      "wherefore art thou shouldst shouldst art wilt thou art a\n",
      "Iteración: 1210, Epoch: 7, Batch: 124 de 182, Pérdida: 5.82\n",
      "Iteración: 1220, Epoch: 7, Batch: 134 de 182, Pérdida: 5.79\n",
      "Iteración: 1230, Epoch: 7, Batch: 144 de 182, Pérdida: 5.63\n",
      "Iteración: 1240, Epoch: 7, Batch: 154 de 182, Pérdida: 5.98\n",
      "Iteración: 1250, Epoch: 7, Batch: 164 de 182, Pérdida: 6.06\n",
      "to be or not be\n",
      "thou art more than\n",
      "wherefore art thou shouldst art restrain'd suff'ring copies slaughter'd companions obdurate interim exchequer\n",
      "Iteración: 1260, Epoch: 7, Batch: 174 de 182, Pérdida: 5.97\n",
      "Epoch actual : 8 / 10\n",
      "Iteración: 1270, Epoch: 8, Batch: 3 de 182, Pérdida: 5.67\n",
      "Iteración: 1280, Epoch: 8, Batch: 13 de 182, Pérdida: 5.89\n",
      "Iteración: 1290, Epoch: 8, Batch: 23 de 182, Pérdida: 5.73\n",
      "Iteración: 1300, Epoch: 8, Batch: 33 de 182, Pérdida: 5.78\n",
      "to be or not be\n",
      "thou art more than that i am a\n",
      "wherefore art thou shouldst art wilt thou art thou art a\n",
      "Iteración: 1310, Epoch: 8, Batch: 43 de 182, Pérdida: 5.81\n",
      "Iteración: 1320, Epoch: 8, Batch: 53 de 182, Pérdida: 6.06\n",
      "Iteración: 1330, Epoch: 8, Batch: 63 de 182, Pérdida: 5.56\n",
      "Iteración: 1340, Epoch: 8, Batch: 73 de 182, Pérdida: 6.11\n",
      "Iteración: 1350, Epoch: 8, Batch: 83 de 182, Pérdida: 5.94\n",
      "to be or not be\n",
      "thou art more than\n",
      "wherefore art thou shouldst art wilt thou art a\n",
      "Iteración: 1360, Epoch: 8, Batch: 93 de 182, Pérdida: 5.85\n",
      "Iteración: 1370, Epoch: 8, Batch: 103 de 182, Pérdida: 5.90\n",
      "Iteración: 1380, Epoch: 8, Batch: 113 de 182, Pérdida: 5.88\n",
      "Iteración: 1390, Epoch: 8, Batch: 123 de 182, Pérdida: 5.77\n",
      "Iteración: 1400, Epoch: 8, Batch: 133 de 182, Pérdida: 5.79\n",
      "to be or not be\n",
      "thou art more than the\n",
      "wherefore art thou shouldst art wilt thou shouldst am a\n",
      "Iteración: 1410, Epoch: 8, Batch: 143 de 182, Pérdida: 5.81\n",
      "Iteración: 1420, Epoch: 8, Batch: 153 de 182, Pérdida: 5.86\n",
      "Iteración: 1430, Epoch: 8, Batch: 163 de 182, Pérdida: 5.86\n",
      "Iteración: 1440, Epoch: 8, Batch: 173 de 182, Pérdida: 5.75\n",
      "Epoch actual : 9 / 10\n",
      "Iteración: 1450, Epoch: 9, Batch: 2 de 182, Pérdida: 5.94\n",
      "to be or not be\n",
      "thou art more than\n",
      "wherefore art thou shouldst art wilt thou art a\n",
      "Iteración: 1460, Epoch: 9, Batch: 12 de 182, Pérdida: 5.73\n",
      "Iteración: 1470, Epoch: 9, Batch: 22 de 182, Pérdida: 6.06\n",
      "Iteración: 1480, Epoch: 9, Batch: 32 de 182, Pérdida: 5.65\n",
      "Iteración: 1490, Epoch: 9, Batch: 42 de 182, Pérdida: 5.97\n",
      "Iteración: 1500, Epoch: 9, Batch: 52 de 182, Pérdida: 5.85\n",
      "Modelo guardado correctamente en ../../datasets/shakespeare/shakespeare_model/model\n",
      "to be or not be\n",
      "thou art more than thou hast\n",
      "wherefore art thou shouldst art wilt thou art a\n",
      "Iteración: 1510, Epoch: 9, Batch: 62 de 182, Pérdida: 5.80\n",
      "Iteración: 1520, Epoch: 9, Batch: 72 de 182, Pérdida: 5.41\n",
      "Iteración: 1530, Epoch: 9, Batch: 82 de 182, Pérdida: 5.60\n",
      "Iteración: 1540, Epoch: 9, Batch: 92 de 182, Pérdida: 5.74\n",
      "Iteración: 1550, Epoch: 9, Batch: 102 de 182, Pérdida: 6.05\n",
      "to be or not be\n",
      "thou art more than the\n",
      "wherefore art thou shouldst art wilt thou art a\n",
      "Iteración: 1560, Epoch: 9, Batch: 112 de 182, Pérdida: 5.82\n",
      "Iteración: 1570, Epoch: 9, Batch: 122 de 182, Pérdida: 5.91\n",
      "Iteración: 1580, Epoch: 9, Batch: 132 de 182, Pérdida: 5.55\n",
      "Iteración: 1590, Epoch: 9, Batch: 142 de 182, Pérdida: 5.62\n",
      "Iteración: 1600, Epoch: 9, Batch: 152 de 182, Pérdida: 5.71\n",
      "to be or not be\n",
      "thou art more than\n",
      "wherefore art thou shouldst art wilt thou shouldst be so much as i\n",
      "Iteración: 1610, Epoch: 9, Batch: 162 de 182, Pérdida: 5.89\n",
      "Iteración: 1620, Epoch: 9, Batch: 172 de 182, Pérdida: 5.81\n",
      "Epoch actual : 10 / 10\n",
      "Iteración: 1630, Epoch: 10, Batch: 1 de 182, Pérdida: 5.95\n",
      "Iteración: 1640, Epoch: 10, Batch: 11 de 182, Pérdida: 5.99\n",
      "Iteración: 1650, Epoch: 10, Batch: 21 de 182, Pérdida: 5.68\n",
      "to be or not be\n",
      "thou art more than the\n",
      "wherefore art thou shouldst art wilt thou art a\n",
      "Iteración: 1660, Epoch: 10, Batch: 31 de 182, Pérdida: 5.79\n",
      "Iteración: 1670, Epoch: 10, Batch: 41 de 182, Pérdida: 5.65\n",
      "Iteración: 1680, Epoch: 10, Batch: 51 de 182, Pérdida: 5.67\n",
      "Iteración: 1690, Epoch: 10, Batch: 61 de 182, Pérdida: 5.74\n",
      "Iteración: 1700, Epoch: 10, Batch: 71 de 182, Pérdida: 6.04\n",
      "to be or not be\n",
      "thou art more than the\n",
      "wherefore art thou shouldst shouldst am glad to see a\n",
      "Iteración: 1710, Epoch: 10, Batch: 81 de 182, Pérdida: 5.71\n",
      "Iteración: 1720, Epoch: 10, Batch: 91 de 182, Pérdida: 5.77\n",
      "Iteración: 1730, Epoch: 10, Batch: 101 de 182, Pérdida: 5.80\n",
      "Iteración: 1740, Epoch: 10, Batch: 111 de 182, Pérdida: 5.71\n",
      "Iteración: 1750, Epoch: 10, Batch: 121 de 182, Pérdida: 5.99\n",
      "to be or not be\n",
      "thou art more than that i have\n",
      "wherefore art thou shouldst art sir john you have not\n",
      "Iteración: 1760, Epoch: 10, Batch: 131 de 182, Pérdida: 6.03\n",
      "Iteración: 1770, Epoch: 10, Batch: 141 de 182, Pérdida: 5.65\n",
      "Iteración: 1780, Epoch: 10, Batch: 151 de 182, Pérdida: 5.81\n",
      "Iteración: 1790, Epoch: 10, Batch: 161 de 182, Pérdida: 5.99\n",
      "Iteración: 1800, Epoch: 10, Batch: 171 de 182, Pérdida: 5.38\n",
      "to be or not be\n",
      "thou art more than the\n",
      "wherefore art thou shouldst am glad to see a\n",
      "Iteración: 1810, Epoch: 10, Batch: 181 de 182, Pérdida: 5.61\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE) as scope:\n",
    "    lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
    "                           training_seq_len, vocab_size)\n",
    "    scope.reuse_variables()\n",
    "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate, \n",
    "                                training_seq_len, vocab_size, infer=True)\n",
    "    \n",
    "    saver = tf.train.Saver()\n",
    "    num_batches = int(len(s_text_vec)/(batch_size*training_seq_len))+1\n",
    "    batches = np.array_split(s_text_vec, num_batches)\n",
    "    batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]### había un for x in batch_size!!! \n",
    "    \n",
    "    session.run(tf.global_variables_initializer())\n",
    "    \n",
    "    train_loss = []\n",
    "    idx_count = 1\n",
    "    \n",
    "    for ep in range(epoch):\n",
    "        random.shuffle(batches)\n",
    "        targets = [np.roll(x, -1, axis=1) for x in batches]\n",
    "        print(\"Epoch actual : \"+str(ep+1)+ \" / \"+str(epoch))\n",
    "        \n",
    "        state = session.run(lstm_model.initial_state)\n",
    "        for ix, batch in enumerate(batches):\n",
    "            training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
    "            c, h = lstm_model.initial_state\n",
    "            training_dict[c] = state.c\n",
    "            training_dict[h] = state.h\n",
    "            \n",
    "            temp_loss, state, _ = session.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_step],\n",
    "                                             feed_dict = training_dict)\n",
    "            train_loss.append(temp_loss)\n",
    "            \n",
    "            if idx_count % 10 == 0:\n",
    "                summary = (idx_count, ep + 1, ix+1, num_batches+1, temp_loss)\n",
    "                print(\"Iteración: {}, Epoch: {}, Batch: {} de {}, Pérdida: {:.2f}\".format(*summary))\n",
    "            if idx_count % save_every == 0:\n",
    "                model_file_name = os.path.join(full_model_dir, \"model\")\n",
    "                saver.save(session, model_file_name, global_step = idx_count)\n",
    "                print(\"Modelo guardado correctamente en {}\".format(model_file_name))\n",
    "                \n",
    "                dict_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
    "                with open(dict_file, 'wb') as dict_file_conn:\n",
    "                    pickle.dump([word2vec, vec2word], dict_file_conn)\n",
    "                    \n",
    "            if idx_count % eval_every == 0:\n",
    "                for sample in prime_texts:\n",
    "                    print(test_lstm_model.sample(session, vec2word, word2vec, num = 10, prime_text = sample))\n",
    "            \n",
    "            idx_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss, 'k-')\n",
    "plt.title(\"Pérdida secuencia a secuencia\")\n",
    "plt.xlabel(\"Iteración\")\n",
    "plt.ylabel(\"Pérdida\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
