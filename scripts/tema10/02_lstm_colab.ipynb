{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "02-lstm_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ty1PJrYakbdp",
        "colab_type": "text"
      },
      "source": [
        "# Clonamos el repositorio para obtener los dataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnS7mdkKkbYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/joanby/tensorflow.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHr76Z3mkbS5",
        "colab_type": "text"
      },
      "source": [
        "# Damos acceso a nuestro Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uaEYTsrkbN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDknN5_tkbI9",
        "colab_type": "text"
      },
      "source": [
        "# Test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbUPQJi5kbEl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/drive/My Drive' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G_PH6rn1ka_7",
        "colab_type": "text"
      },
      "source": [
        "# Google colab tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwn95VBwka6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "import glob # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "from google.colab import drive # Montar tu Google drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KnXYAXika1l",
        "colab_type": "text"
      },
      "source": [
        "##Especificando la versión de TensorFlow\n",
        "\n",
        "Ejecutando \"importar tensorflow\" importará la versión por defecto (actualmente 2.x). Puedes usar la 1.x ejecutando una celda con la \"versión mágica de tensorflow\" **antes de ejecutar \"importar tensorflow\".\n",
        "\n",
        "### Si no funciona hacer el pip install\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1qV0j1Wkav8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow==1.14\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rABrJP_mkapa",
        "colab_type": "text"
      },
      "source": [
        "# Importar Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVSMSMvWkago",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdGLTewYkacS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZgq3I-wkZTq",
        "colab_type": "text"
      },
      "source": [
        "# Long Short Term Memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5-4N7QtkZTr",
        "colab_type": "text"
      },
      "source": [
        "$$i_t = \\sigma(B_ih_{t-1}+A_ix_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oj-FLCs9kZTs",
        "colab_type": "text"
      },
      "source": [
        "$$C_t = \\tanh(B_ch_{t-1}+A_cx_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2Cjw6ZUkZTt",
        "colab_type": "text"
      },
      "source": [
        "$$f_t = \\sigma(B_fh_{t-1}+A_fx_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iuD7rfebkZTt",
        "colab_type": "text"
      },
      "source": [
        "$$N_t = i_t\\cdot C_t +f_t N_{t-1}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0P8uU5GkZTt",
        "colab_type": "text"
      },
      "source": [
        "$$O_t = \\sigma(B_oh_{t-1}+A_ox_t+D_oN_t)$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzBL3aDJkZTu",
        "colab_type": "text"
      },
      "source": [
        "$$h_t = O_t\\cdot \\tanh(N_t)$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clnLxoQxkZTu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import requests\n",
        "import numpy as np\n",
        "import collections\n",
        "import random\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmdSPid2kZTy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dir = \"/content/tensorflow/datasets/shakespeare\"\n",
        "data_file = \"shakespeare.txt\"\n",
        "model_path = \"shakespeare_model\"\n",
        "full_model_dir = os.path.join(data_dir, model_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMfuP2nokZT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgNbKX81kZT3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "punctuation = ''.join([x for x in string.punctuation if x not in ['-', \"'\"]])\n",
        "punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zh1hDC_hkZT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if not os.path.exists(full_model_dir):\n",
        "    os.makedirs(full_model_dir)\n",
        "\n",
        "if not os.path.isfile(os.path.join(data_dir, data_file)):\n",
        "    url = \"http://www.gutenberg.org/cache/epub/100/pg100.txt\"\n",
        "    response = requests.get(url)\n",
        "    s_text = response.content.decode('utf-8')\n",
        "    s_text = s_text[7675:]\n",
        "    s_text = s_text.replace(\"\\r\\n\", '')\n",
        "    s_text = s_text.replace(\"\\n\",'')\n",
        "    \n",
        "    with open(os.path.join(data_dir, data_file), \"w\") as file_out:\n",
        "        file_out.write(s_text)\n",
        "else:\n",
        "    with open(os.path.join(data_dir, data_file), \"r\") as file_out:\n",
        "        s_text = file_out.read().replace(\"\\n\",'')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E5aB9yfBkZT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_text = re.sub(r'[{}]'.format(punctuation), ' ', s_text)\n",
        "s_text = re.sub('\\s+', ' ', s_text).strip().lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZKER7mEkZT9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEVWyVU7kZT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session()\n",
        "\n",
        "min_word_freq = 5\n",
        "rnn_size = 128\n",
        "epoch = 10\n",
        "batch_size = 100\n",
        "learning_rate = 0.001\n",
        "training_seq_len = 50\n",
        "embedding_size = rnn_size\n",
        "save_every = 500\n",
        "eval_every = 50\n",
        "\n",
        "prime_texts = ['to be or not be', 'thou art more', 'wherefore art thou']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QSU9vWV7kZUB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_vocabulary(text, min_word_freq):\n",
        "    word_count = collections.Counter(text.split(' '))\n",
        "    word_count = {key:val for key, val in word_count.items() if val>min_word_freq}\n",
        "    words = word_count.keys()\n",
        "    word_to_vec = {key:(ix+1) for ix, key in enumerate(words)}\n",
        "    word_to_vec['unknown'] = 0\n",
        "    vec_to_word = {val:key for key, val in word_to_vec.items()}\n",
        "    return vec_to_word, word_to_vec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICrk3WnykZUD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vec2word, word2vec = build_vocabulary(s_text, min_word_freq)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlwlcMLUkZUF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vec2word)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7dBLf9tkZUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRjDMLWCkZUL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "s_text_words = s_text.split(' ')\n",
        "s_text_vec = []\n",
        "for ix, x in enumerate(s_text_words):\n",
        "    try: \n",
        "        s_text_vec.append(word2vec[x])\n",
        "    except:\n",
        "        s_text_vec.append(0)\n",
        "s_text_vec = np.array(s_text_vec)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMx6goIukZUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LSTM_Model():\n",
        "    def __init__(self, embedding_size, rnn_size, batch_size, learning_rate, ## embedding_size\n",
        "                 training_seq_length, vocabulary_size, infer = False ):\n",
        "        \n",
        "        self.embedding_size = embedding_size ### guardar en self el objeto embedding_size\n",
        "        \n",
        "        self.rnn_size = rnn_size\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.infer = infer\n",
        "        self.learning_rate = learning_rate\n",
        "        \n",
        "        if infer:\n",
        "            self.batch_size = 1\n",
        "            self.training_seq_len = 1\n",
        "        else:\n",
        "            self.batch_size = batch_size\n",
        "            self.training_seq_len = training_seq_len\n",
        "            \n",
        "        self.lstm_cell = tf.contrib.rnn.BasicLSTMCell(self.rnn_size) ### cambiado por contrib!!!\n",
        "        self.initial_state = self.lstm_cell.zero_state(self.batch_size, tf.float32)\n",
        "        \n",
        "        self.x_data = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
        "        self.y_output = tf.placeholder(tf.int32, [self.batch_size, self.training_seq_len])\n",
        "\n",
        "        \n",
        "        with tf.variable_scope('lstm_vars'):\n",
        "            weigths = tf.get_variable('weigths', [self.rnn_size, self.vocabulary_size], tf.float32,\n",
        "                                tf.random_normal_initializer())\n",
        "            bias = tf.get_variable('bias', [self.vocabulary_size], tf.float32,\n",
        "                                tf.constant_initializer(0.0))\n",
        "            \n",
        "            embedding_mat = tf.get_variable('embedding_mat', [self.vocabulary_size, self.embedding_size],\n",
        "                                            tf.float32, tf.random_normal_initializer())\n",
        "            \n",
        "            embedding_output = tf.nn.embedding_lookup(embedding_mat, self.x_data)\n",
        "            rnn_inputs = tf.split(axis=1, num_or_size_splits=self.training_seq_len, value=embedding_output)\n",
        "            rnn_inputs_trimmed = [tf.squeeze(x, [1]) for x in rnn_inputs]\n",
        "            \n",
        "        def inferred_loop(prev, count):\n",
        "            prev_trans = tf.add(tf.matmul(prev, weights), bias)\n",
        "            prev_symbol = tf.stop_gradient(tf.argmax(prev_trans, 1))\n",
        "            output = tf.nn.embedding_lookup(embedding_mat, prev_symbol)\n",
        "            \n",
        "        decoder = tf.contrib.legacy_seq2seq.rnn_decoder\n",
        "        outputs, last_state = decoder(rnn_inputs_trimmed, ### FALTABA UNA S\n",
        "                                      self.initial_state,\n",
        "                                      self.lstm_cell,\n",
        "                                      loop_function=inferred_loop if infer else None)\n",
        "        \n",
        "        output = tf.reshape(tf.concat(axis=1, values=outputs), [-1, self.rnn_size])\n",
        "        \n",
        "        self.logit_output = tf.add(tf.matmul(output, weigths),bias)\n",
        "        self.model_output = tf.nn.softmax(self.logit_output)\n",
        "        \n",
        "        loss_func = tf.contrib.legacy_seq2seq.sequence_loss_by_example\n",
        "        loss = loss_func([self.logit_output],[tf.reshape(self.y_output, [-1])],\n",
        "                [tf.ones([self.batch_size * self.training_seq_len])],\n",
        "                self.vocabulary_size)\n",
        "        \n",
        "        self.cost = tf.reduce_sum(loss) / (self.batch_size * self.training_seq_len)\n",
        "        self.final_state = last_state\n",
        "\n",
        "        gradients, _ = tf.clip_by_global_norm(tf.gradients(self.cost, tf.trainable_variables()), 4.5)\n",
        "        \n",
        "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        self.train_step = optimizer.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
        "\n",
        "        \n",
        "        \n",
        "    def sample(self, session, words = vec2word, vocab = word2vec, num = 10, prime_text = 'thou art'):\n",
        "        state = session.run(self.lstm_cell.zero_state(1, tf.float32))\n",
        "        word_list = prime_text.split()\n",
        "        for word in word_list[:-1]:\n",
        "            x = np.zeros((1,1))\n",
        "            x[0,0] = vocab[word]\n",
        "            feed_dict = {self.x_data: x, self.initial_state:state}\n",
        "            [state] = session.run([self.final_state], feed_dict=feed_dict)\n",
        "        \n",
        "        out_sentence = prime_text\n",
        "        word = word_list[-1]\n",
        "        for n in range(num):\n",
        "            x = np.zeros((1,1))\n",
        "            x[0,0] = vocab[word]\n",
        "            feed_dict = {self.x_data: x, self.initial_state: state}\n",
        "            [model_output, state] = session.run([self.model_output, self.final_state], feed_dict=feed_dict)\n",
        "            sample = np.argmax(model_output[0])\n",
        "            if sample == 0:\n",
        "                break\n",
        "            word = words[sample]\n",
        "            out_sentence = out_sentence + \" \" + word\n",
        "        return out_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "D107xCBpkZUR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE) as scope:\n",
        "    lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate,\n",
        "                           training_seq_len, vocab_size)\n",
        "    scope.reuse_variables()\n",
        "    test_lstm_model = LSTM_Model(embedding_size, rnn_size, batch_size, learning_rate, \n",
        "                                training_seq_len, vocab_size, infer=True)\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    num_batches = int(len(s_text_vec)/(batch_size*training_seq_len))+1\n",
        "    batches = np.array_split(s_text_vec, num_batches)\n",
        "    batches = [np.resize(x, [batch_size, training_seq_len]) for x in batches]### había un for x in batch_size!!! \n",
        "    \n",
        "    session.run(tf.global_variables_initializer())\n",
        "    \n",
        "    train_loss = []\n",
        "    idx_count = 1\n",
        "    \n",
        "    for ep in range(epoch):\n",
        "        random.shuffle(batches)\n",
        "        targets = [np.roll(x, -1, axis=1) for x in batches]\n",
        "        print(\"Epoch actual : \"+str(ep+1)+ \" / \"+str(epoch))\n",
        "        \n",
        "        state = session.run(lstm_model.initial_state)\n",
        "        for ix, batch in enumerate(batches):\n",
        "            training_dict = {lstm_model.x_data: batch, lstm_model.y_output: targets[ix]}\n",
        "            c, h = lstm_model.initial_state\n",
        "            training_dict[c] = state.c\n",
        "            training_dict[h] = state.h\n",
        "            \n",
        "            temp_loss, state, _ = session.run([lstm_model.cost, lstm_model.final_state, lstm_model.train_step],\n",
        "                                             feed_dict = training_dict)\n",
        "            train_loss.append(temp_loss)\n",
        "            \n",
        "            if idx_count % 10 == 0:\n",
        "                summary = (idx_count, ep + 1, ix+1, num_batches+1, temp_loss)\n",
        "                print(\"Iteración: {}, Epoch: {}, Batch: {} de {}, Pérdida: {:.2f}\".format(*summary))\n",
        "            if idx_count % save_every == 0:\n",
        "                model_file_name = os.path.join(full_model_dir, \"model\")\n",
        "                saver.save(session, model_file_name, global_step = idx_count)\n",
        "                print(\"Modelo guardado correctamente en {}\".format(model_file_name))\n",
        "                \n",
        "                dict_file = os.path.join(full_model_dir, 'vocab.pkl')\n",
        "                with open(dict_file, 'wb') as dict_file_conn:\n",
        "                    pickle.dump([word2vec, vec2word], dict_file_conn)\n",
        "                    \n",
        "            if idx_count % eval_every == 0:\n",
        "                for sample in prime_texts:\n",
        "                    print(test_lstm_model.sample(session, vec2word, word2vec, num = 10, prime_text = sample))\n",
        "            \n",
        "            idx_count += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nr2NiWnTkZUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(train_loss, 'k-')\n",
        "plt.title(\"Pérdida secuencia a secuencia\")\n",
        "plt.xlabel(\"Iteración\")\n",
        "plt.ylabel(\"Pérdida\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}