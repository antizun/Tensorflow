{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "04-cbow_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u95xeudJWMQ",
        "colab_type": "text"
      },
      "source": [
        "# Clonamos el repositorio para obtener los dataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YkBQH_YbJhq1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "7c8fe327-63be-43e3-fb78-8e07451ebaa6"
      },
      "source": [
        "!git clone https://github.com/joanby/tensorflow.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tensorflow'...\n",
            "remote: Enumerating objects: 51, done.\u001b[K\n",
            "remote: Counting objects: 100% (51/51), done.\u001b[K\n",
            "remote: Compressing objects: 100% (28/28), done.\u001b[K\n",
            "remote: Total 60311 (delta 32), reused 37 (delta 23), pack-reused 60260\u001b[K\n",
            "Receiving objects: 100% (60311/60311), 442.46 MiB | 29.69 MiB/s, done.\n",
            "Resolving deltas: 100% (82/82), done.\n",
            "Checking out files: 100% (60225/60225), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "li2tQF2IJdxs",
        "colab_type": "text"
      },
      "source": [
        "# Damos acceso a nuestro Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xn7AmVbCJm-_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjqkjByIJekE",
        "colab_type": "text"
      },
      "source": [
        "# Test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgEcTQV0JfHp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/drive/My Drive' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUuRDgJRJfbW",
        "colab_type": "text"
      },
      "source": [
        "# Google colab tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpBk1bQ-JsZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "import glob # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "from google.colab import drive # Montar tu Google drive"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aa-YkU94JvMB",
        "colab_type": "text"
      },
      "source": [
        "##Especificando la versión de TensorFlow\n",
        "\n",
        "Ejecutando \"importar tensorflow\" importará la versión por defecto (actualmente 2.x). Puedes usar la 1.x ejecutando una celda con la \"versión mágica de tensorflow\" **antes de ejecutar \"importar tensorflow\".\n",
        "\n",
        "### Si no funciona hacer el pip install\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8j6B5ySJsSq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ef0fd42-0935-444d-bfb8-d5e055bc29c3"
      },
      "source": [
        "#!pip install tensorflow==1.14\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jY-XY16oJx_-",
        "colab_type": "text"
      },
      "source": [
        "# Importar Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXD_2zi6J0aA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "362befc9-8d89-4237-ecdf-c00569217ac9"
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.15.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSr1Tk3LJ2TS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoBEL2-BJReS",
        "colab_type": "text"
      },
      "source": [
        "# Continuous Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5D-GEuuHJReT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "import string\n",
        "import requests\n",
        "import collections\n",
        "import io\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import nltk\n",
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9MPsLG8JReX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDEXEpDLJReZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_movies_data():\n",
        "    save_folder_name = \"/content/tensorflow/datasets/movies_data\"\n",
        "    pos_file = os.path.join(save_folder_name, 'rt-polarity.pos')\n",
        "    neg_file = os.path.join(save_folder_name, 'rt-polarity.neg')\n",
        "    \n",
        "    if os.path.exists(save_folder_name):\n",
        "        ## Podemos cargar la info directamente desde el PC\n",
        "        pos_data = []\n",
        "        with open(pos_file, 'r') as temp_pos_file:\n",
        "            for row in temp_pos_file:\n",
        "                pos_data.append(row)\n",
        "                \n",
        "        neg_data = []\n",
        "        with open(neg_file, 'r') as temp_neg_file:\n",
        "            for row in temp_neg_file:\n",
        "                neg_data.append(row)\n",
        "        \n",
        "    else:\n",
        "        ## Debemos descargar los ficheros de internet y guardarlos en esta carpeta\n",
        "        url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\"\n",
        "        stream_data = urllib.request.urlopen(url)\n",
        "        tmp = io.BytesIO()\n",
        "        while True:\n",
        "            s = stream_data.read(16384)\n",
        "            if not s: \n",
        "                break\n",
        "            tmp.write(s)\n",
        "        stream_data.close()\n",
        "        tmp.seek(0)\n",
        "        \n",
        "        tar_file = tarfile.open(fileobj=tmp, mode='r:gz')\n",
        "        pos = tar_file.extractfile('rt-polaritydata/rt-polarity.pos')\n",
        "        neg = tar_file.extractfile('rt-polaritydata/rt-polarity.neg')\n",
        "        \n",
        "        pos_data = []\n",
        "        for line in pos:\n",
        "            pos_data.append(line.decode('ISO-8859-1').encode('ascii', errors='ignore').decode())\n",
        "            \n",
        "        neg_data = []\n",
        "        for line in neg:\n",
        "            neg_data.append(line.decode('ISO-8859-1').encode('ascii', errors='ignore').decode())\n",
        "            \n",
        "        tar_file.close()\n",
        "        \n",
        "        if not os.path.exists(save_folder_name):\n",
        "            os.makedirs(save_folder_name)\n",
        "        with open(pos_file, 'w') as pos_file_handler:\n",
        "            pos_file_handler.write(''.join(pos_data))\n",
        "        with open(neg_file, 'w') as neg_file_handler:\n",
        "            neg_file_handler.write(''.join(neg_data))\n",
        "    \n",
        "    texts = pos_data + neg_data\n",
        "    target = [1]*len(pos_data) + [0]*len(neg_data)\n",
        "    return (texts, target)         "
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lTt7trLkJRec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_text(texts, stops):\n",
        "    texts = [x.lower() for x in texts]\n",
        "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
        "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
        "    texts = [' '.join(word for word in x.split() if word not in (stops)) for x in texts]\n",
        "    texts = [' '.join(x.split()) for x in texts]\n",
        "    return texts"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIWbw_eNJRee",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dictionary(sentences, vocabulary_size):\n",
        "    split_sentences = [s.split() for s in sentences]\n",
        "    words = [x for sublist in split_sentences for x in sublist]\n",
        "    count = [['RARE', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
        "    word_dict = {}\n",
        "    for word, word_count in count:\n",
        "        word_dict[word] = len(word_dict)\n",
        "    return word_dict"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WE7-stbtJReg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_numbers(sentences, word_dict):\n",
        "    data = []\n",
        "    for sentence in sentences:\n",
        "        sentence_data = []\n",
        "        for word in sentence:\n",
        "            if word in word_dict:\n",
        "                word_ix = word_dict[word]#posición/ID de la palabra en el word dict\n",
        "            else:\n",
        "                word_ix = 0 ##posición/ID de la palabra RARE\n",
        "            sentence_data.append(word_ix)\n",
        "        data.append(sentence_data)\n",
        "    return data"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPsftwEtJRei",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch_data(sentences, batch_size, window_size, method = 'skip_gram'):\n",
        "    '''\n",
        "        Skip Gram: Mi perro come su comida -> (Mi, come), (perro, come), (su, come), (comida, come)\n",
        "        Cbow: Mi perro come su comida -> ([Mi,perro,su,comida]; come)\n",
        "    '''\n",
        "    batch_data = []\n",
        "    label_data = []\n",
        "    \n",
        "    while len(batch_data) < batch_size:\n",
        "        rand_sentences = np.random.choice(sentences)\n",
        "        window_seq = [rand_sentences[max((ix-window_size),0):(ix+window_size+1)] \n",
        "                      for ix, x in enumerate(rand_sentences)]\n",
        "        label_idx = [ix if ix < window_size else window_size for ix, x in enumerate(window_seq)]\n",
        "        \n",
        "        if method == 'skip_gram':\n",
        "            batch_and_labels = [(x[y], x[:y]+x[(y+1):]) for x,y in zip(window_seq, label_idx)]\n",
        "            # Convertir el dato a una lista de tuplas (palabra objetivo, contexto)\n",
        "            tuple_data = [(x,y_) for x, y in batch_and_labels for y_ in y]\n",
        "            batch, labels = [list(x) for x in zip(*tuple_data)]\n",
        "\n",
        "        elif method=='cbow':\n",
        "            batch_and_labels = [(x[:y]+x[(y+1):], x[y]) for x,y in zip(window_seq, label_idx)]\n",
        "            # Conservar las ventanas de tamaño 2*window_size\n",
        "            batch_and_labels = [(x,y) for x,y in batch_and_labels if len(x)==2*window_size]\n",
        "            \n",
        "            if len(batch_and_labels) <1:\n",
        "                continue\n",
        "            \n",
        "            batch, labels = [list(x) for x in zip(*batch_and_labels)]\n",
        "            \n",
        "        else:\n",
        "            raise ValueError(\"Método {} no implementado\".format(method))\n",
        "        \n",
        "        batch_data.extend(batch[:batch_size])\n",
        "        label_data.extend(labels[:batch_size])\n",
        "        \n",
        "    batch_data = batch_data[:batch_size]\n",
        "    label_data = label_data[:batch_size]\n",
        "    \n",
        "    batch_data = np.array(batch_data)\n",
        "    label_data = np.transpose(np.array([label_data]))\n",
        "    \n",
        "    return (batch_data, label_data)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDS-LWTVJRek",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "b7bee9b7-6b79-40e8-a3c8-54f99762ee7d"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "batch_size = 500\n",
        "embedding_size = 200\n",
        "vocabulary_size = 2000\n",
        "generations = 10000\n",
        "model_learning_rate = 0.001\n",
        "num_sampled = int(batch_size/2)\n",
        "window_size = 3\n",
        "\n",
        "save_embeddings_every = 500\n",
        "print_valid_every = 1000\n",
        "print_loss_every = 200\n",
        "\n",
        "stops = stopwords.words('english')\n",
        "valid_words = ['love', 'hate', 'happy', 'sad', 'man', 'woman']"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3y_fVCVXJRem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts, target = load_movies_data()\n",
        "texts = normalize_text(texts, stops)"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MVM5inkfJReo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_dictionary = build_dictionary(texts, vocabulary_size)\n",
        "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
        "text_data = text_to_numbers(texts, word_dictionary)\n",
        "valid_examples = [word_dictionary[x] for x in valid_words]"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJgNXBOEJRet",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "9c18d917-5239-4502-fa6e-b4a427ddff3f"
      },
      "source": [
        "print(valid_words)\n",
        "print(valid_examples)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['love', 'hate', 'happy', 'sad', 'man', 'woman']\n",
            "[28, 895, 812, 359, 132, 582]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mwgeU1gJRev",
        "colab_type": "text"
      },
      "source": [
        "### Entrenar el modelo"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izx7UWiAJRev",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "x_inputs = tf.placeholder(tf.int32, shape = [batch_size, 2*window_size])\n",
        "y_target = tf.placeholder(tf.int32, shape = [batch_size, 1])\n",
        "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gYXY6yaJRex",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed = tf.zeros([batch_size, embedding_size])\n",
        "for element in range(2*window_size):\n",
        "    embed += tf.nn.embedding_lookup(embeddings, x_inputs[:, element])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_rR3nUliJRez",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], \n",
        "                                              stddev=1.0/np.sqrt(embedding_size)))\n",
        "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIc-11TZJRe1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "d064289d-2f70-45a4-840d-716c879da030"
      },
      "source": [
        "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, biases=nce_biases,\n",
        "                                    inputs=embed, labels=y_target, \n",
        "                                     num_sampled = num_sampled, num_classes=vocabulary_size))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iKPe3IGcJRe3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1, keepdims=True))\n",
        "normalized_embeddings = embeddings/norm\n",
        "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvO9pD_NJRe5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "saver = tf.train.Saver({\"embeddings\": embeddings})"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqD_mr-TJRe7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=model_learning_rate).minimize(loss)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uf78WqMSJRe9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "session.run(init)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l43_ebEzJRe_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_vect = []\n",
        "loss_x_vect = []\n",
        "\n",
        "for i in range(generations):\n",
        "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size, method='cbow')\n",
        "    feed_dict = {x_inputs: batch_inputs, y_target: batch_labels}\n",
        "    session.run(optimizer, feed_dict=feed_dict)\n",
        "    \n",
        "    if (i+1) % print_loss_every == 0:\n",
        "        loss_val = session.run(loss, feed_dict=feed_dict)\n",
        "        loss_vect.append(loss_val)\n",
        "        loss_x_vect.append(i+1)\n",
        "        print(\"Iteración {}, Pérdida: \".format(i+1, loss_val))\n",
        "    \n",
        "    ## Validación del contexto, imprimiremos algunas palabras aleatorias y su top 5\n",
        "    if (i+1) % print_valid_every == 0:\n",
        "        sim = session.run(similarity, feed_dict=feed_dict)\n",
        "        for j in range(len(valid_words)):\n",
        "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
        "            top_k = 5\n",
        "            nearest = (-sim[j,:]).argsort()[1:top_k+1]\n",
        "            log_string = \"Palabras cercanas a {}:\".format(valid_word)\n",
        "            for k in range(top_k):\n",
        "                close_word = word_dictionary_rev[nearest[k]]\n",
        "                log_string = \"%s %s, \"%(log_string, close_word)\n",
        "            print(log_string)\n",
        "    if (i+1) % save_embeddings_every == 0:\n",
        "        #Guardar el vocabulario de palabras\n",
        "        with open(os.path.join('../../datasets', 'movie_vocabulary.pkl'), 'wb') as f:\n",
        "            pickle.dump(word_dictionary, f)\n",
        "        #Guardar los embeddings\n",
        "        model_checkpoint_path = os.path.join(\"../../datasets\", \"cbow_movie_embeddings.ckpt\")\n",
        "        save_path = saver.save(session, model_checkpoint_path)\n",
        "        print(\"Modelo guardado en el fichero: {}\".format(save_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-klzwOESJRfB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(loss_x_vect, loss_vect, 'r-', label=\"Función de pérdidas de CBOW\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}