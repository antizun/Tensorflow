{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.5"
    },
    "colab": {
      "name": "03-skip-grams_colab.ipynb",
      "provenance": []
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UC9i9yjHlls",
        "colab_type": "text"
      },
      "source": [
        "# Clonamos el repositorio para obtener los dataSet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pgq0Pzd-HmNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/joanby/tensorflow.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ymYI56RHy2x",
        "colab_type": "text"
      },
      "source": [
        "# Damos acceso a nuestro Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UACpK6I_HmLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGpZO39pH3Iv",
        "colab_type": "text"
      },
      "source": [
        "# Test it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SLJwnePHmIj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!ls '/content/drive/My Drive' "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1uM64tGH6Dy",
        "colab_type": "text"
      },
      "source": [
        "# Google colab tools"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUPlctE3HmDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import files # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "import glob # Para manejar los archivos y, por ejemplo, exportar a su navegador\n",
        "from google.colab import drive # Montar tu Google drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbqFqPM1H8v1",
        "colab_type": "text"
      },
      "source": [
        "##Especificando la versión de TensorFlow\n",
        "\n",
        "Ejecutando \"importar tensorflow\" importará la versión por defecto (actualmente 2.x). Puedes usar la 1.x ejecutando una celda con la \"versión mágica de tensorflow\" **antes de ejecutar \"importar tensorflow\".\n",
        "\n",
        "### Si no funciona hacer el pip install\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox4sIoFgHmAm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install tensorflow==1.14\n",
        "%tensorflow_version 1.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VMR8tCIIAf8",
        "colab_type": "text"
      },
      "source": [
        "# Importar Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gliXKmvHl94",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PYB7arusID-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "session = tf.Session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8ZFc12DHjK3",
        "colab_type": "text"
      },
      "source": [
        "# Bag of Words Contínuo o Skip Gramas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7btXjfG_HjK4",
        "colab_type": "text"
      },
      "source": [
        "- King - man + woman = Queen\n",
        "- Indian Pale Ale - hops + malt = Stout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8SRv9-ZFHjK4",
        "colab_type": "text"
      },
      "source": [
        "- Si queremos predecir una palabra objetivo a partir de un contexto (palabras que la rodean): Continuous Bag of Words\n",
        "- Si queremos predecir las palabras que rodean (contexto) a una palabra objetivo: Skip-Grama"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFjk5fFIHjK5",
        "colab_type": "text"
      },
      "source": [
        "## Descarga de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pQ5ZmH4MHjK5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import string\n",
        "import requests\n",
        "import collections\n",
        "import io\n",
        "import tarfile\n",
        "import urllib.request\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "session = tf.Session()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ubcfs0SUHjK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 50\n",
        "embedding_size = 200\n",
        "vocabulary_size = 10000\n",
        "generations = 50000\n",
        "print_loss_every = 500\n",
        "num_sampled = int(batch_size/2)\n",
        "window_size = 2\n",
        "nltk.download('stopwords')\n",
        "stops = stopwords.words('english')\n",
        "print_valid_every = 2000\n",
        "valid_words = ['cliche', 'love', 'hate', 'silly', 'sad']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6bV3HOpRHjK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stops"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNyLTi-RHjLB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_movies_data():\n",
        "    save_folder_name = \"../../datasets/movies_data\"\n",
        "    pos_file = os.path.join(save_folder_name, 'rt-polarity.pos')\n",
        "    neg_file = os.path.join(save_folder_name, 'rt-polarity.neg')\n",
        "    \n",
        "    if os.path.exists(save_folder_name):\n",
        "        ## Podemos cargar la info directamente desde el PC\n",
        "        pos_data = []\n",
        "        with open(pos_file, 'r') as temp_pos_file:\n",
        "            for row in temp_pos_file:\n",
        "                pos_data.append(row)\n",
        "                \n",
        "        neg_data = []\n",
        "        with open(neg_file, 'r') as temp_neg_file:\n",
        "            for row in temp_neg_file:\n",
        "                neg_data.append(row)\n",
        "        \n",
        "    else:\n",
        "        ## Debemos descargar los ficheros de internet y guardarlos en esta carpeta\n",
        "        url = \"http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz\"\n",
        "        stream_data = urllib.request.urlopen(url)\n",
        "        tmp = io.BytesIO()\n",
        "        while True:\n",
        "            s = stream_data.read(16384)\n",
        "            if not s: \n",
        "                break\n",
        "            tmp.write(s)\n",
        "        stream_data.close()\n",
        "        tmp.seek(0)\n",
        "        \n",
        "        tar_file = tarfile.open(fileobj=tmp, mode='r:gz')\n",
        "        pos = tar_file.extractfile('rt-polaritydata/rt-polarity.pos')\n",
        "        neg = tar_file.extractfile('rt-polaritydata/rt-polarity.neg')\n",
        "        \n",
        "        pos_data = []\n",
        "        for line in pos:\n",
        "            pos_data.append(line.decode('ISO-8859-1').encode('ascii', errors='ignore').decode())\n",
        "            \n",
        "        neg_data = []\n",
        "        for line in neg:\n",
        "            neg_data.append(line.decode('ISO-8859-1').encode('ascii', errors='ignore').decode())\n",
        "            \n",
        "        tar_file.close()\n",
        "        \n",
        "        if not os.path.exists(save_folder_name):\n",
        "            os.makedirs(save_folder_name)\n",
        "        with open(pos_file, 'w') as pos_file_handler:\n",
        "            pos_file_handler.write(''.join(pos_data))\n",
        "        with open(neg_file, 'w') as neg_file_handler:\n",
        "            neg_file_handler.write(''.join(neg_data))\n",
        "    \n",
        "    texts = pos_data + neg_data\n",
        "    target = [1]*len(pos_data) + [0]*len(neg_data)\n",
        "    return (texts, target)         "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "butefYBfHjLD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts, target = load_movies_data()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxgkjCQjHjLF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QghpsRpHjLI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6RJ-ANsHjLK",
        "colab_type": "text"
      },
      "source": [
        "## Limpieza de datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdWOYzqwHjLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize_text(texts, stops):\n",
        "    texts = [x.lower() for x in texts]\n",
        "    texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
        "    texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
        "    texts = [' '.join(word for word in x.split() if word not in (stops)) for x in texts]\n",
        "    texts = [' '.join(x.split()) for x in texts]\n",
        "    return texts"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_4Z-NxbHjLN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts = normalize_text(texts, stops)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dS7InX74HjLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-VAlx6LoHjLR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OLcZ3CtmHjLT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "target = [target[ix] for ix, x in enumerate(texts) if len(x.split())>2]\n",
        "texts = [x for x in texts if len(x.split())>2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA_RA97YHjLV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "len(texts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZIvxZNtHjLX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dictionary(sentences, vocabulary_size):\n",
        "    split_sentences = [s.split() for s in sentences]\n",
        "    words = [x for sublist in split_sentences for x in sublist]\n",
        "    count = [['RARE', -1]]\n",
        "    count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
        "    word_dict = {}\n",
        "    for word, word_count in count:\n",
        "        word_dict[word] = len(word_dict)\n",
        "    return word_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MW0s6WknHjLZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_dict = build_dictionary(texts, vocabulary_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ0kjnfVHjLb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGqF0-KHHjLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def text_to_numbers(sentences, word_dict):\n",
        "    data = []\n",
        "    for sentence in sentences:\n",
        "        sentence_data = []\n",
        "        for word in sentence:\n",
        "            if word in word_dict:\n",
        "                word_ix = word_dict[word]#posición/ID de la palabra en el word dict\n",
        "            else:\n",
        "                word_ix = 0 ##posición/ID de la palabra RARE\n",
        "            sentence_data.append(word_ix)\n",
        "        data.append(sentence_data)\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGrN0GhsHjLf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_dict_rev = dict(zip(word_dict.values(), word_dict.keys()))\n",
        "word_dict_rev"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh7XD_rxHjLj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_data = text_to_numbers(texts, word_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "SpaElaUHHjLl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text_data[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfXDKTbBHjLm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_examples = [word_dict[x] for x in valid_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YikfAJi5HjLo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "valid_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxrlSMRxHjLq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_batch_data(sentences, batch_size, window_size, method = 'skip_gram'):\n",
        "    '''\n",
        "        Mi perro come su comida -> (Mi, come), (perro, come), (su, come), (comida, come)\n",
        "    '''\n",
        "    batch_data = []\n",
        "    label_data = []\n",
        "    \n",
        "    while len(batch_data) < batch_size:\n",
        "        rand_sentences = np.random.choice(sentences)\n",
        "        window_seq = [rand_sentences[max((ix-window_size),0):(ix+window_size+1)] \n",
        "                      for ix, x in enumerate(rand_sentences)]\n",
        "        label_idx = [ix if ix < window_size else window_size for ix, x in enumerate(window_seq)]\n",
        "        \n",
        "        if method == 'skip_gram':\n",
        "            batch_and_labels = [(x[y], x[:y]+x[(y+1):]) for x,y in zip(window_seq, label_idx)]\n",
        "            tuple_data = [(x,y_) for x, y in batch_and_labels for y_ in y]\n",
        "        else:\n",
        "            raise ValueError(\"Método {} no implementado\".format(method))\n",
        "        \n",
        "        batch, labels = [list(x) for x in zip(*tuple_data)]\n",
        "        batch_data.extend(batch[:batch_size])\n",
        "        label_data.extend(labels[:batch_size])\n",
        "        \n",
        "    batch_data = batch_data[:batch_size]\n",
        "    label_data = label_data[:batch_size]\n",
        "    \n",
        "    batch_data = np.array(batch_data)\n",
        "    label_data = np.transpose(np.array([label_data]))\n",
        "    \n",
        "    return (batch_data, label_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7OqOZ3cUHjLs",
        "colab_type": "text"
      },
      "source": [
        "## Entrenar con los Skip Grams"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RPBveus6HjLs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1, 1))\n",
        "x_inputs = tf.placeholder(tf.int32, shape =[batch_size])\n",
        "y_target = tf.placeholder(tf.int32, shape = [batch_size,1])\n",
        "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M9KADTtUHjLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embed = tf.nn.embedding_lookup(embeddings, x_inputs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3qAV0rxvHjLw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nce_weights = tf.Variable(tf.truncated_normal([vocabulary_size, embedding_size], \n",
        "                                              stddev=1.0/np.sqrt(embedding_size)))\n",
        "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TChs68rAHjLy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights, biases=nce_biases,\n",
        "                                    inputs=embed, labels=y_target, \n",
        "                                     num_sampled = num_sampled, num_classes=vocabulary_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYtlGk0VHjL0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings),1, keepdims=True))\n",
        "normalized_embeddings = embeddings/norm\n",
        "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGLbpRdNHjL1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=1.0).minimize(loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NX30FRM2HjL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "session.run(init)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRBz2GVZHjL5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "loss_vect = []\n",
        "loss_x_vect = []\n",
        "for i in range(generations):\n",
        "    batch_inputs, batch_labels = generate_batch_data(text_data, batch_size, window_size)\n",
        "    feed_dict = {x_inputs: batch_inputs, y_target: batch_labels}\n",
        "    session.run(optimizer, feed_dict=feed_dict)\n",
        "    \n",
        "    if (i+1) % print_loss_every == 0:\n",
        "        loss_val = session.run(loss, feed_dict=feed_dict)\n",
        "        loss_vect.append(loss_val)\n",
        "        loss_x_vect.append(i+1)\n",
        "        print(\"Iteración {}, Pérdida: \".format(i+1, loss_val))\n",
        "    \n",
        "    ## Validación de palabras más cercanas a las 5 seleccionadas\n",
        "    if (i+1) % print_valid_every == 0:\n",
        "        sim = session.run(similarity, feed_dict=feed_dict)\n",
        "        for j in range(len(valid_words)):\n",
        "            valid_word = word_dict_rev[valid_examples[j]]\n",
        "            top_k = 10\n",
        "            nearest = (-sim[j,:]).argsort()[1:top_k+1]\n",
        "            log_string = \"Palabras cercanas a {}:\".format(valid_word)\n",
        "            for k in range(top_k):\n",
        "                close_word = word_dict_rev[nearest[k]]\n",
        "                log_string = \"%s %s, \"%(log_string, close_word)\n",
        "            print(log_string)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1tyFrgw5HjL6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.plot(loss_x_vect, loss_vect, 'k-', label = \"Función de pérdidas\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O9hycw49HjL8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "texts[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnqTCrQqHjL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "word_dict[\"rock\"], word_dict[\"destined\"], word_dict[\"st\"], word_dict[\"new\"], word_dict[\"conan\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqcxLb1QHjMA",
        "colab_type": "text"
      },
      "source": [
        "- Si tomamos la palabra \"destined\" como objetivo, es la número 2457\n",
        "- Con una ventana de dimensión 1 -> 535 y 2251"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j58T-EB3HjMA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M = session.run(embeddings)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebsJyo5RHjMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M[2457]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjG_fzyGHjMD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "M[535]"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}